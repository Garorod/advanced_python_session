{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gensim\n",
    "import json\n",
    "import nltk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction from each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 330\n"
     ]
    }
   ],
   "source": [
    "tweets_file = open(\"../dataset/1502820001-tweets.txt\", 'r')\n",
    "lines = tweets_file.readlines()\n",
    "print (\"Number of tweets: %d\" % len(lines))\n",
    "tweets_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'contributors': None,\n",
      " 'coordinates': None,\n",
      " 'created_at': 'Tue Aug 15 17:58:26 +0000 2017',\n",
      " 'entities': {'hashtags': [{'indices': [45, 52], 'text': 'patent'},\n",
      "                           {'indices': [65, 68], 'text': 'IP'}],\n",
      "              'symbols': [{'indices': [103, 108], 'text': 'GOOG'},\n",
      "                          {'indices': [109, 112], 'text': 'FB'}],\n",
      "              'urls': [{'display_url': 'iam-media.com/blog/Detail.as…',\n",
      "                        'expanded_url': 'http://www.iam-media.com/blog/Detail.aspx?g=afc6cc58-706a-475d-906a-fd85bd1e49f1',\n",
      "                        'indices': [113, 136],\n",
      "                        'url': 'https://t.co/FiHWRiETq3'}],\n",
      "              'user_mentions': [{'id': 108564136,\n",
      "                                 'id_str': '108564136',\n",
      "                                 'indices': [3, 16],\n",
      "                                 'name': 'IAM',\n",
      "                                 'screen_name': 'IAM_magazine'}]},\n",
      " 'favorite_count': 0,\n",
      " 'favorited': False,\n",
      " 'geo': None,\n",
      " 'id': 897517856702812160,\n",
      " 'id_str': '897517856702812160',\n",
      " 'in_reply_to_screen_name': None,\n",
      " 'in_reply_to_status_id': None,\n",
      " 'in_reply_to_status_id_str': None,\n",
      " 'in_reply_to_user_id': None,\n",
      " 'in_reply_to_user_id_str': None,\n",
      " 'is_quote_status': False,\n",
      " 'lang': 'en',\n",
      " 'metadata': {'iso_language_code': 'en', 'result_type': 'recent'},\n",
      " 'place': None,\n",
      " 'possibly_sensitive': False,\n",
      " 'retweet_count': 9,\n",
      " 'retweeted': False,\n",
      " 'retweeted_status': {'contributors': None,\n",
      "                      'coordinates': None,\n",
      "                      'created_at': 'Mon Aug 14 18:37:19 +0000 2017',\n",
      "                      'entities': {'hashtags': [{'indices': [27, 34],\n",
      "                                                 'text': 'patent'},\n",
      "                                                {'indices': [47, 50],\n",
      "                                                 'text': 'IP'}],\n",
      "                                   'symbols': [{'indices': [85, 90],\n",
      "                                                'text': 'GOOG'},\n",
      "                                               {'indices': [91, 94],\n",
      "                                                'text': 'FB'}],\n",
      "                                   'urls': [{'display_url': 'iam-media.com/blog/Detail.as…',\n",
      "                                             'expanded_url': 'http://www.iam-media.com/blog/Detail.aspx?g=afc6cc58-706a-475d-906a-fd85bd1e49f1',\n",
      "                                             'indices': [95, 118],\n",
      "                                             'url': 'https://t.co/FiHWRiETq3'}],\n",
      "                                   'user_mentions': []},\n",
      "                      'favorite_count': 4,\n",
      "                      'favorited': False,\n",
      "                      'geo': None,\n",
      "                      'id': 897165251254382593,\n",
      "                      'id_str': '897165251254382593',\n",
      "                      'in_reply_to_screen_name': None,\n",
      "                      'in_reply_to_status_id': None,\n",
      "                      'in_reply_to_status_id_str': None,\n",
      "                      'in_reply_to_user_id': None,\n",
      "                      'in_reply_to_user_id_str': None,\n",
      "                      'is_quote_status': False,\n",
      "                      'lang': 'en',\n",
      "                      'metadata': {'iso_language_code': 'en',\n",
      "                                   'result_type': 'recent'},\n",
      "                      'place': None,\n",
      "                      'possibly_sensitive': False,\n",
      "                      'retweet_count': 9,\n",
      "                      'retweeted': False,\n",
      "                      'source': '<a href=\"http://twitter.com\" '\n",
      "                                'rel=\"nofollow\">Twitter Web Client</a>',\n",
      "                      'text': 'Exclusive: In major Valley #patent move Google '\n",
      "                              '#IP head Allen Lo is joining Facebook $GOOG $FB '\n",
      "                              'https://t.co/FiHWRiETq3',\n",
      "                      'truncated': False,\n",
      "                      'user': {'contributors_enabled': False,\n",
      "                               'created_at': 'Tue Jan 26 09:55:36 +0000 2010',\n",
      "                               'default_profile': False,\n",
      "                               'default_profile_image': False,\n",
      "                               'description': 'Please note: Retweets should '\n",
      "                                              'not be regarded as '\n",
      "                                              'endorsements.',\n",
      "                               'entities': {'description': {'urls': []},\n",
      "                                            'url': {'urls': [{'display_url': 'iam-media.com',\n",
      "                                                              'expanded_url': 'http://www.iam-media.com',\n",
      "                                                              'indices': [0,\n",
      "                                                                          22],\n",
      "                                                              'url': 'http://t.co/lSHJSRwQa1'}]}},\n",
      "                               'favourites_count': 128,\n",
      "                               'follow_request_sent': False,\n",
      "                               'followers_count': 5320,\n",
      "                               'following': False,\n",
      "                               'friends_count': 269,\n",
      "                               'geo_enabled': True,\n",
      "                               'has_extended_profile': False,\n",
      "                               'id': 108564136,\n",
      "                               'id_str': '108564136',\n",
      "                               'is_translation_enabled': False,\n",
      "                               'is_translator': False,\n",
      "                               'lang': 'en',\n",
      "                               'listed_count': 210,\n",
      "                               'location': 'London-Hong Kong-Washington DC',\n",
      "                               'name': 'IAM',\n",
      "                               'notifications': False,\n",
      "                               'profile_background_color': 'DFDFDF',\n",
      "                               'profile_background_image_url': 'http://pbs.twimg.com/profile_background_images/536860588987514881/whGihSNG.png',\n",
      "                               'profile_background_image_url_https': 'https://pbs.twimg.com/profile_background_images/536860588987514881/whGihSNG.png',\n",
      "                               'profile_background_tile': False,\n",
      "                               'profile_banner_url': 'https://pbs.twimg.com/profile_banners/108564136/1416832322',\n",
      "                               'profile_image_url': 'http://pbs.twimg.com/profile_images/536860397869887488/ddZRusGP_normal.png',\n",
      "                               'profile_image_url_https': 'https://pbs.twimg.com/profile_images/536860397869887488/ddZRusGP_normal.png',\n",
      "                               'profile_link_color': 'F11B23',\n",
      "                               'profile_sidebar_border_color': 'FFFFFF',\n",
      "                               'profile_sidebar_fill_color': 'DDEEF6',\n",
      "                               'profile_text_color': '333333',\n",
      "                               'profile_use_background_image': True,\n",
      "                               'protected': False,\n",
      "                               'screen_name': 'IAM_magazine',\n",
      "                               'statuses_count': 15380,\n",
      "                               'time_zone': 'London',\n",
      "                               'translator_type': 'none',\n",
      "                               'url': 'http://t.co/lSHJSRwQa1',\n",
      "                               'utc_offset': 3600,\n",
      "                               'verified': False}},\n",
      " 'source': '<a href=\"http://twitter.com\" rel=\"nofollow\">Twitter Web Client</a>',\n",
      " 'text': 'RT @IAM_magazine: Exclusive: In major Valley #patent move Google #IP '\n",
      "         'head Allen Lo is joining Facebook $GOOG $FB https://t.co/FiHWRiETq3',\n",
      " 'truncated': False,\n",
      " 'user': {'contributors_enabled': False,\n",
      "          'created_at': 'Wed May 02 22:00:01 +0000 2007',\n",
      "          'default_profile': True,\n",
      "          'default_profile_image': False,\n",
      "          'description': 'Developer, patent lawyer, typo checker. Tweets tend '\n",
      "                         'to be about Apple, IP, faith, truth, beauty, '\n",
      "                         'mundanity. I work for @parallel_tw.',\n",
      "          'entities': {'description': {'urls': []}},\n",
      "          'favourites_count': 8369,\n",
      "          'follow_request_sent': False,\n",
      "          'followers_count': 1257,\n",
      "          'following': False,\n",
      "          'friends_count': 3520,\n",
      "          'geo_enabled': True,\n",
      "          'has_extended_profile': False,\n",
      "          'id': 5725012,\n",
      "          'id_str': '5725012',\n",
      "          'is_translation_enabled': False,\n",
      "          'is_translator': False,\n",
      "          'lang': 'en',\n",
      "          'listed_count': 101,\n",
      "          'location': 'Boston, MA',\n",
      "          'name': 'Michael Saji',\n",
      "          'notifications': False,\n",
      "          'profile_background_color': 'C0DEED',\n",
      "          'profile_background_image_url': 'http://abs.twimg.com/images/themes/theme1/bg.png',\n",
      "          'profile_background_image_url_https': 'https://abs.twimg.com/images/themes/theme1/bg.png',\n",
      "          'profile_background_tile': False,\n",
      "          'profile_banner_url': 'https://pbs.twimg.com/profile_banners/5725012/1394588981',\n",
      "          'profile_image_url': 'http://pbs.twimg.com/profile_images/613751130439286784/neIdTi5y_normal.png',\n",
      "          'profile_image_url_https': 'https://pbs.twimg.com/profile_images/613751130439286784/neIdTi5y_normal.png',\n",
      "          'profile_link_color': '1DA1F2',\n",
      "          'profile_sidebar_border_color': 'C0DEED',\n",
      "          'profile_sidebar_fill_color': 'DDEEF6',\n",
      "          'profile_text_color': '333333',\n",
      "          'profile_use_background_image': True,\n",
      "          'protected': False,\n",
      "          'screen_name': 'saji',\n",
      "          'statuses_count': 12722,\n",
      "          'time_zone': 'Eastern Time (US & Canada)',\n",
      "          'translator_type': 'none',\n",
      "          'url': None,\n",
      "          'utc_offset': -14400,\n",
      "          'verified': False}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(json.loads(lines[0].strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO use all twitter files\n",
    "data = defaultdict(dict)\n",
    "i=0\n",
    "for line in lines:\n",
    "\n",
    "    tweet = json.loads(line.strip())\n",
    "    if 'text' in tweet: # only messages contains 'text' field is a tweet\n",
    "        ts = time.strptime(tweet['created_at'],'%a %b %d %H:%M:%S +0000 %Y')\n",
    "        data[i][\"time\"] = time.mktime(ts)  \n",
    "        data[i][\"text\"] = tweet['text']\n",
    "    if 'urls' in tweet['entities']:\n",
    "        #print tweet['entities']['urls']\n",
    "        data[i][\"urls\"] = len(tweet['entities']['urls'])\n",
    "    if 'hashtags' in tweet['entities']:\n",
    "        data[i][\"hashtags\"] = len(tweet['entities']['hashtags'])\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'time': 1502834306.0, 'text': 'RT @IAM_magazine: Exclusive: In major Valley #patent move Google #IP head Allen Lo is joining Facebook $GOOG $FB https://t.co/FiHWRiETq3', 'urls': 1, 'hashtags': 2}\n",
      "{'time': 1502834158.0, 'text': 'RT @arnabch01: #investors massive bubble in #tech be careful $AAPL $GOOG $MSFT $AMZN $FB $NFLX $TSLA $CSCO $INTC $NVDA $ZNGA $ORCL $JD $MU…', 'urls': 0, 'hashtags': 2}\n"
     ]
    }
   ],
   "source": [
    "### which other signals could be useful? \n",
    "print (data[0])\n",
    "print (data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'bytes' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0a64883089ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'ignore'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# content of the tweet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"http\\S*\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#remove urls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr\"^rt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#remove rt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'bytes' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "#working with text\n",
    "#tokenizer for tweets\n",
    "tknzr = TweetTokenizer(strip_handles=True) #(strip_handles=True, reduce_len=True)\n",
    "corpus = []\n",
    "for i, info in data.items():  \n",
    "    text = info['text'].lower()\n",
    "    text = text.encode('utf-8').decode('ascii','ignore') # content of the tweet\n",
    "    text = re.sub(r\"http\\S*\", '', text) #remove urls\n",
    "    text = re.sub(r\"^rt\", '', text) #remove rt\n",
    "    text = text.replace('#', '') #remove hashtag\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    words = tknzr.tokenize(text)\n",
    "    text = \" \".join(words).encode('utf-8')\n",
    "\n",
    "    if text not in corpus:\n",
    "        corpus.append(text)\n",
    "        data[i]['text'] = text\n",
    "        data[i]['exclamations'] = words.count('!')\n",
    "        data[i]['questions'] = words.count('?')\n",
    "        data[i]['dollar'] = words.count('$')\n",
    "        data[i]['num_words'] = len(text) \n",
    "    else:\n",
    "        data.pop(i)\n",
    "    \n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'urls': 1, 'text': 'iam_magazine exclusive in major valley patent move google ip head allen lo is joining facebook goog fb', 'hashtags': 2, 'dollar': 0, 'questions': 0, 'time': 1502845106.0, 'exclamations': 0, 'num_words': 102}\n",
      "{'urls': 0, 'text': 'arnabch investors massive bubble in tech be careful aapl goog msft amzn fb nflx tsla csco intc nvda znga orcl jd mu', 'hashtags': 2, 'dollar': 0, 'questions': 0, 'time': 1502844958.0, 'exclamations': 0, 'num_words': 115}\n",
      "203\n"
     ]
    }
   ],
   "source": [
    "print (data[0])\n",
    "print (data[1])\n",
    "print (len(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>dollar</th>\n",
       "      <th>questions</th>\n",
       "      <th>time</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>203.000000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>203.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>2.030000e+02</td>\n",
       "      <td>203.0</td>\n",
       "      <td>203.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.709360</td>\n",
       "      <td>0.566502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.502820e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.743842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486716</td>\n",
       "      <td>1.643849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.744604e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.452337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.502506e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.502841e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.502843e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             urls    hashtags  dollar  questions          time  exclamations  \\\n",
       "count  203.000000  203.000000   203.0      203.0  2.030000e+02         203.0   \n",
       "mean     0.709360    0.566502     0.0        0.0  1.502820e+09           0.0   \n",
       "std      0.486716    1.643849     0.0        0.0  6.744604e+04           0.0   \n",
       "min      0.000000    0.000000     0.0        0.0  1.502506e+09           0.0   \n",
       "25%      0.000000    0.000000     0.0        0.0  1.502841e+09           0.0   \n",
       "50%      1.000000    0.000000     0.0        0.0  1.502843e+09           0.0   \n",
       "75%      1.000000    0.000000     0.0        0.0  1.502845e+09           0.0   \n",
       "max      2.000000   10.000000     0.0        0.0  1.502845e+09           0.0   \n",
       "\n",
       "        num_words  \n",
       "count  203.000000  \n",
       "mean    80.743842  \n",
       "std     26.452337  \n",
       "min     23.000000  \n",
       "25%     62.000000  \n",
       "50%     81.000000  \n",
       "75%    103.000000  \n",
       "max    131.000000  "
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 203 entries, 0 to 326\n",
      "Data columns (total 8 columns):\n",
      "urls            203 non-null int64\n",
      "text            203 non-null object\n",
      "hashtags        203 non-null int64\n",
      "dollar          203 non-null int64\n",
      "questions       203 non-null int64\n",
      "time            203 non-null float64\n",
      "exclamations    203 non-null int64\n",
      "num_words       203 non-null int64\n",
      "dtypes: float64(1), int64(6), object(1)\n",
      "memory usage: 14.3+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction from Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text features based on frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates\n",
    "df = df.drop_duplicates(subset=['text'], keep=False)\n",
    "df.describe()\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         frequency\n",
      "count  1078.000000\n",
      "mean      1.969388\n",
      "std       3.433408\n",
      "min       1.000000\n",
      "25%       1.000000\n",
      "50%       1.000000\n",
      "75%       2.000000\n",
      "max      49.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aap</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapl</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abnormalreturns</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abound</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accelerating</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>account</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acquired</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acquires</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acquisition</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>action</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 frequency\n",
       "aap                      2\n",
       "aapl                    34\n",
       "abnormalreturns          1\n",
       "abound                   1\n",
       "accelerating             1\n",
       "account                  1\n",
       "acquired                 1\n",
       "acquires                 5\n",
       "acquisition              1\n",
       "action                   1"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "word_vectorizer = CountVectorizer(analyzer='word', stop_words='english')\n",
    "sparse_matrix = word_vectorizer.fit_transform(df['text'])\n",
    "frequencies = sum(sparse_matrix).toarray()[0]\n",
    "words = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n",
    "print words.describe()\n",
    "words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smaller dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        frequency\n",
      "count  346.000000\n",
      "mean     3.994220\n",
      "std      5.541932\n",
      "min      2.000000\n",
      "25%      2.000000\n",
      "50%      2.000000\n",
      "75%      4.000000\n",
      "max     49.000000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frequency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>aap</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aapl</th>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>acquires</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ads</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advances</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>advisors</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>affect</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ago</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ai</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alny</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          frequency\n",
       "aap               2\n",
       "aapl             34\n",
       "acquires          5\n",
       "ads               3\n",
       "advances          2\n",
       "advisors          2\n",
       "affect            2\n",
       "ago               2\n",
       "ai                7\n",
       "alny              2"
      ]
     },
     "execution_count": 518,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectorizer = CountVectorizer(analyzer='word', stop_words='english',min_df=2, max_df=3000)\n",
    "sparse_matrix = word_vectorizer.fit_transform(df['text'])\n",
    "frequencies = sum(sparse_matrix).toarray()[0]\n",
    "words = pd.DataFrame(frequencies, index=word_vectorizer.get_feature_names(), columns=['frequency'])\n",
    "print words.describe()\n",
    "words.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding structure in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "del words\n",
    "#create data_samples\n",
    "#data_samples= [t['text'] for t in data.values()]\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'aap', u'aapl', u'acquires', u'ads', u'advances', u'advisors', u'affect', u'ago', u'ai', u'alny', u'alphabet', u'amazon', u'ameystone', u'amp', u'amzn', u'analysis', u'apple', u'apples', u'aprn', u'armonk', u'arnabch', u'arranged', u'ashburton', u'ask', u'asrockinfo', u'asset', u'augmented', u'august', u'auto', u'azure', u'ba', u'bac', u'bank', u'bargaining', u'barronsonline', u'big', u'bigdata', u'bitcf', u'blue', u'bond']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "n_features = 250\n",
    "n_components = 10\n",
    "n_top_words = 10\n",
    "\n",
    "\n",
    "### Counts\n",
    "tf_vectorizer = CountVectorizer(min_df=2, max_df=1000, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(df.text)\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print tf_feature_names[:40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF as text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 521,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "## TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=2,max_df=1000,stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(df.text)\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding topics with LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ibm box business international machines million position corporation china investors\n",
      "Topic #1: snap buy ibm stocks fb fraud detailed analysis csco like\n",
      "Topic #2: snap amzn box exploding mr marketing expense snaps amazon amp\n",
      "Topic #3: box week spy xiv review dan lt alny timfundamentals deux\n",
      "Topic #4: box snap check technologies chkp software point critical fitzgerald sees\n",
      "Topic #5: goog arnabch ai ml robotics msft bigdata ibm lift rbc\n",
      "Topic #6: box ibm goog googl ipo shares aapl know dropbox need\n",
      "Topic #7: ibm ginni blue china yield killing strategic imperatives dividend vz\n",
      "Topic #8: msft aapl amzn box ms tsla goog acquires gs nflx\n",
      "Topic #9: amzn aapl fb nflx spy box amp goog data twtr\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, learning_method='online')\n",
    "lda.fit(tf) ## fitting counts\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ibm buy goog dividend snap right news vz good safest\n",
      "Topic #1: chart ago volume paying rising ibm bargaining search default iphone\n",
      "Topic #2: snap buy fb detailed analysis box exploding marketing expense snaps\n",
      "Topic #3: ibm ginni blue box china arranged tuesday fraud week rev\n",
      "Topic #4: box aapl ipo googl upcoming dropbox check need know goog\n",
      "Topic #5: arnabch msft ai ml goog robotics snap bigdata iot amzn\n",
      "Topic #6: amzn box amazon stock acquires msft ms new aapl gs\n",
      "Topic #7: box business amp international position million expected machines somewhat delivery\n",
      "Topic #8: shares box plc buys total amp making sold alphabet group\n",
      "Topic #9: amzn snap think ibm risks hd stocks aprn market watson\n"
     ]
    }
   ],
   "source": [
    "lda = LatentDirichletAllocation(n_components=n_components, learning_method='online')\n",
    "lda.fit(tfidf) ## fitting tf-idf counts\n",
    "print_top_words(lda, tfidf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "positive = pd.read_csv('positive-words.txt', names=['a'])\n",
    "positive =  set(positive['a'].tolist())\n",
    "\n",
    "negative = pd.read_csv('negative-words.txt', names=['a'])\n",
    "negative =  set(negative['a'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "count_positive = []\n",
    "count_negative = []\n",
    "for i, row in df.iterrows():\n",
    "    commonp = set(row['text'].split()).intersection(positive) \n",
    "    count_positive.append(len(commonp))\n",
    "    commonn = set(row['text'].split()).intersection(negative) \n",
    "    count_negative.append(len(commonn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>text</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>dollar</th>\n",
       "      <th>questions</th>\n",
       "      <th>time</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>num_words</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>iam_magazine exclusive in major valley patent ...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>arnabch investors massive bubble in tech be ca...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>nyinvesting google goog is the embodiment of m...</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>114</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>greenstocks timberr iwm spy tlt gs gld btc goo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>bank of nova scotia buys shares of alphabet in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>alphabet inc goog stake raised by north star a...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>themotleyfool the machines keep getting smarte...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>as alphabet goog valuation rose robshaw amp ju...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502844e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>warren averett asset management llc boosts pos...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502844e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>goog himx vuzi great article</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.502844e+09</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   urls                                               text  hashtags  dollar  \\\n",
       "0     1  iam_magazine exclusive in major valley patent ...         2       0   \n",
       "1     0  arnabch investors massive bubble in tech be ca...         2       0   \n",
       "2     0  nyinvesting google goog is the embodiment of m...         6       0   \n",
       "3     0  greenstocks timberr iwm spy tlt gs gld btc goo...         0       0   \n",
       "4     1  bank of nova scotia buys shares of alphabet in...         0       0   \n",
       "5     1  alphabet inc goog stake raised by north star a...         0       0   \n",
       "6     1  themotleyfool the machines keep getting smarte...         0       0   \n",
       "7     0  as alphabet goog valuation rose robshaw amp ju...         0       0   \n",
       "8     1  warren averett asset management llc boosts pos...         0       0   \n",
       "9     1                       goog himx vuzi great article         0       0   \n",
       "\n",
       "   questions          time  exclamations  num_words  positive  negative  \n",
       "0          0  1.502845e+09             0        102         0         0  \n",
       "1          0  1.502845e+09             0        115         0         0  \n",
       "2          0  1.502845e+09             0        114         2         1  \n",
       "3          0  1.502845e+09             0        106         0         0  \n",
       "4          0  1.502845e+09             0         52         0         0  \n",
       "5          0  1.502845e+09             0         65         0         0  \n",
       "6          0  1.502845e+09             0         94         2         0  \n",
       "7          0  1.502844e+09             0         88         0         0  \n",
       "8          0  1.502844e+09             0         72         0         0  \n",
       "9          0  1.502844e+09             0         28         1         0  "
      ]
     },
     "execution_count": 526,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['positive'] = count_positive\n",
    "df['negative'] = count_negative\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>urls</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>dollar</th>\n",
       "      <th>questions</th>\n",
       "      <th>time</th>\n",
       "      <th>exclamations</th>\n",
       "      <th>num_words</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>203.000000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>203.0</td>\n",
       "      <td>203.0</td>\n",
       "      <td>2.030000e+02</td>\n",
       "      <td>203.0</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>203.000000</td>\n",
       "      <td>203.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.709360</td>\n",
       "      <td>0.566502</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.502820e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>80.743842</td>\n",
       "      <td>0.413793</td>\n",
       "      <td>0.310345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.486716</td>\n",
       "      <td>1.643849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.744604e+04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26.452337</td>\n",
       "      <td>0.649605</td>\n",
       "      <td>0.594730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.502506e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.502841e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.502843e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.502845e+09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             urls    hashtags  dollar  questions          time  exclamations  \\\n",
       "count  203.000000  203.000000   203.0      203.0  2.030000e+02         203.0   \n",
       "mean     0.709360    0.566502     0.0        0.0  1.502820e+09           0.0   \n",
       "std      0.486716    1.643849     0.0        0.0  6.744604e+04           0.0   \n",
       "min      0.000000    0.000000     0.0        0.0  1.502506e+09           0.0   \n",
       "25%      0.000000    0.000000     0.0        0.0  1.502841e+09           0.0   \n",
       "50%      1.000000    0.000000     0.0        0.0  1.502843e+09           0.0   \n",
       "75%      1.000000    0.000000     0.0        0.0  1.502845e+09           0.0   \n",
       "max      2.000000   10.000000     0.0        0.0  1.502845e+09           0.0   \n",
       "\n",
       "        num_words    positive    negative  \n",
       "count  203.000000  203.000000  203.000000  \n",
       "mean    80.743842    0.413793    0.310345  \n",
       "std     26.452337    0.649605    0.594730  \n",
       "min     23.000000    0.000000    0.000000  \n",
       "25%     62.000000    0.000000    0.000000  \n",
       "50%     81.000000    0.000000    0.000000  \n",
       "75%    103.000000    1.000000    0.500000  \n",
       "max    131.000000    3.000000    3.000000  "
      ]
     },
     "execution_count": 527,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6      themotleyfool the machines keep getting smarte...\n",
       "9                           goog himx vuzi great article\n",
       "10     robertrelder apples bargaining power rising go...\n",
       "14     arnabch ai robotics bigdata genomics stemcell ...\n",
       "15     applewatch to support both lte and nonlte mode...\n",
       "20     beijing transit contactless mpayment system ex...\n",
       "26     tweaktown pr asrockinfo introduces the x iot r...\n",
       "29     pr asrockinfo introduces the x iot router for ...\n",
       "41     stocktwits since its ipo home depot is actuall...\n",
       "45     edborgato amzns same day pick up locations are...\n",
       "49     would be amazed if jana partners manage to sel...\n",
       "56           gs aapl amzn need to lead us higher spx dji\n",
       "58     active traders try one of these free trading g...\n",
       "59     xplr join us for play by play action on stocks...\n",
       "60     amzn pzza restaurants are in a tech race to ma...\n",
       "68     amzn part bmark offering guidance y y y y y y ...\n",
       "73     hot options alert midday tuesday august bac dk...\n",
       "77     there is a chance apple could be making a doub...\n",
       "80     join robinhoodapp and well both get a share of...\n",
       "81     retail never learns when buying stock you dont...\n",
       "84     energyandcapita why investors should love the ...\n",
       "86     tsla has traded volume of yesterday and nearly...\n",
       "88     a collection of testimonials from satisfied su...\n",
       "89     why investors should love the new tsla semitru...\n",
       "93     lisahopeking dont be too proud to copy is fb i...\n",
       "94     fb new facebook data center a boost to ohios t...\n",
       "96     amazing tandemtrader is amazing if you want to...\n",
       "98     boost your focus with these simple exercises y...\n",
       "108    the real reason ibm is like a utility csco d d...\n",
       "124    china lchaim make me a match ginni cant even d...\n",
       "138    arnabch quantum computing t revolutionize ai m...\n",
       "140                           ibm a good opportunity ibm\n",
       "143    barronstechblog amazon baird likes hulu win ex...\n",
       "152    pretty basic set up aezs lets see what where s...\n",
       "157    ltea huge news out mybeststock stockinfotv frs...\n",
       "162    af_singledad digaf wake up people grab you a s...\n",
       "163    aapl fb twtr snap googl tech companies urge su...\n",
       "166    digaf wake up people grab you a sure fire winn...\n",
       "170    androsform opening lines id be looking at righ...\n",
       "186    top internet stocks on the market amzn ebay gr...\n",
       "187    cah bandicoot available for ps and box buy now...\n",
       "190    first trust advisors lp raises stake in box in...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 528,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['positive'] >0) & (df['negative']  == 0)]['text'].head(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18     goog neonazi group moves to dark web after web...\n",
       "30     arnabch will advances in ai ml robotics nanote...\n",
       "33     arnabch hpc ai ml bigdata may soon enable geno...\n",
       "39     discussing the retail landscape department sto...\n",
       "48     sitrep risk on mrk ceo youre fired amzn gs leg...\n",
       "54     amzn aap wmt amazon will probably go onto crus...\n",
       "55     dont worry about how many shares you can buy c...\n",
       "67     thestreet amazon will probably go onto crush a...\n",
       "83     tsla sa another risk factor for tesla shorts d...\n",
       "99     microsoft acquires cloudcomputing orchestratio...\n",
       "115    international business machines ibm fall to no...\n",
       "117             the blue cloud collapses i told u ibm so\n",
       "121    jimcramer mariabartiromo so u wont ask ginni a...\n",
       "122    seekingalpha ibm watson disappointment risks f...\n",
       "123    ibm watson disappointment risks further downwa...\n",
       "128    china big market thus saith ginni so far zero ...\n",
       "131    marketsupchuck is ibms dividend yield killing ...\n",
       "132    is ibms dividend yield killing strategic imper...\n",
       "149    stocknewsdotcom snap cantor fitzgerald sees al...\n",
       "150    snap cantor fitzgerald sees almost upside for ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[(df['positive'] ==0) & (df['negative']  > 0)]['text'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tweets: 203\n",
      "Total tweets positive: 42\n",
      "Total tweets negative: 25\n",
      "Tweets with no info: 110\n",
      "neutral tweets: 26\n"
     ]
    }
   ],
   "source": [
    "print \"Total tweets:\", len(df)\n",
    "print \"Total tweets positive:\",len(df[(df['positive'] >0) & (df['negative']  == 0)])\n",
    "print \"Total tweets negative:\",len(df[(df['positive'] == 0) & (df['negative']  > 0)])\n",
    "print \"Tweets with no info:\", len(df[(df['positive'] == 0) & (df['negative']  == 0)])\n",
    "print \"neutral tweets:\", len(df[(df['positive'] >0) & (df['negative']  > 0)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content Similarity using word embedings (Word2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting each tweet into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "matrix = []\n",
    "filtered = []\n",
    "for i, row in data.items():\n",
    "    filtered_text = [model[w] for w in row['text'].split() if w in model and w not in stop_words]\n",
    "    filtered.append([w for w in row['text'].split() if w in model and w not in stop_words])\n",
    "    if len(filtered_text):\n",
    "        matrix.append(matutils.unitvec(np.array(filtered_text).mean(axis=0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.672333    0.60287177 ...,  0.37708838  0.24777096\n",
      "   0.41357933]\n",
      " [ 0.672333    1.          0.61664081 ...,  0.39533368  0.30348312\n",
      "   0.38047786]\n",
      " [ 0.60287177  0.61664081  1.         ...,  0.40589874  0.23713568\n",
      "   0.37125429]\n",
      " ..., \n",
      " [ 0.37708838  0.39533368  0.40589874 ...,  1.          0.5730864\n",
      "   0.52884238]\n",
      " [ 0.24777096  0.30348312  0.23713568 ...,  0.5730864   1.          0.64343482]\n",
      " [ 0.41357933  0.38047786  0.37125429 ...,  0.52884238  0.64343482  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#computing similarity between tweets\n",
    "\n",
    "matrix = np.array(matrix)\n",
    "sim = np.dot(matrix, matrix.transpose())\n",
    "print sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(203, 203)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20503.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.392697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.121137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.068056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.313049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.388658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.470452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0\n",
       "count  20503.000000\n",
       "mean       0.392697\n",
       "std        0.121137\n",
       "min       -0.068056\n",
       "25%        0.313049\n",
       "50%        0.388658\n",
       "75%        0.470452\n",
       "max        1.000000"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reshaping into a data frame\n",
    "print sim.shape\n",
    "dup = np.fill_diagonal(sim, 0)\n",
    "\n",
    "simdf = pd.DataFrame(list(sim[np.triu_indices(sim.shape[1], 1)]))\n",
    "simdf.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the most similar tweets for each sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 0.63219642533\n",
      "stocktwits since its ipo home depot is actually outperforming amazon compare the green to the yellow line on this\n",
      "['since', 'ipo', 'home', 'depot', 'actually', 'outperforming', 'amazon', 'compare', 'green', 'yellow', 'line']\n",
      "barronstechblog amazon baird likes hulu win expanding tool set barrons tech trader daily amzn googl msft ibm\n",
      "['amazon', 'baird', 'likes', 'hulu', 'win', 'expanding', 'tool', 'set', 'tech', 'trader', 'daily', 'msft', 'ibm']\n"
     ]
    }
   ],
   "source": [
    "pos = 41\n",
    "most_similar = np.argmax(sim[pos][:])\n",
    "print \"similarity:\", sim[pos][most_similar]\n",
    "print df.iloc[pos]['text']\n",
    "print filtered[pos]\n",
    "print df.iloc[most_similar]['text']\n",
    "print filtered[most_similar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: 0.70566082456\n",
      "dont worry about how many shares you can buy concern yourself wthe return on those shares stocks amzn googl\n",
      "['dont', 'worry', 'many', 'shares', 'buy', 'concern', 'wthe', 'return', 'shares', 'stocks']\n",
      "retail never learns when buying stock you dont buy high and sell low thats what theyre doing right now with aapl good luck\n",
      "['retail', 'never', 'learns', 'buying', 'stock', 'dont', 'buy', 'high', 'sell', 'low', 'thats', 'theyre', 'right', 'aapl', 'good', 'luck']\n"
     ]
    }
   ],
   "source": [
    "neg = 55\n",
    "most_similar = np.argmax(sim[neg][:])\n",
    "print \"similarity:\", sim[neg][most_similar]\n",
    "print df.iloc[neg]['text']\n",
    "print filtered[neg]\n",
    "print df.iloc[most_similar]['text']\n",
    "print filtered[most_similar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
